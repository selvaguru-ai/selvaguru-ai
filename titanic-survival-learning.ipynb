{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n\"\"\"\nSelva - notes\nonehot_encode = OneHotEncoder()\nprint (onehot_encode)\n\n#one hot encoding the categorical values\nfeature_array = onehot_encode.fit_transform(X[[\"Pclass\",\"Sex\",\"SibSp\",\"Parch\",\"Cabin\",\"Embarked\"]]).toarray()\n\n#labelling the columns with the categorical features\nfeature_labels = onehot_encode.categories_\n#now we have to flaten the list since feature_labels has nested list in it\nflattened_label = []\nfor sublist in feature_labels:\n    for items in sublist:\n        flattened_label.append(items)\n\nprint (flattened_label)\n#np.array(feature_labels).ravel()\n\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-19T14:58:45.132284Z","iopub.execute_input":"2024-06-19T14:58:45.133261Z","iopub.status.idle":"2024-06-19T14:58:45.155952Z","shell.execute_reply.started":"2024-06-19T14:58:45.133223Z","shell.execute_reply":"2024-06-19T14:58:45.154573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Understanding the Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ntrain_data = pd.read_csv('/kaggle/input/titanic/train.csv')\nprint (train_data.columns)\n\n#Embarked has only 2 NaN values so removing the rows now\ntrain_data = train_data.dropna(subset = [\"Embarked\"])\n#Also dropping Text features since they are not useful in this classification\n#X = X.drop(columns = [\"PassengerId\", \"Name\", \"Ticket\"])\n\n#splitting the training data into Y (predicted variable) and X \n#(Independent variable - predictor of X)\nX = train_data.drop(['Survived'], axis = 1)\nY = train_data['Survived']\n\nprint (X.shape)\n\n#print (\"Independent variables(X): \", X[:4])\n#print (\"To be predicted variables(Y): \", Y[:4])\n\n# Check for NaN values in each column\nnan_columns = X.columns[X.isna().any()].tolist()\nprint (X['Age'].isna().sum())\nprint (X['Cabin'].isna().sum())\nprint (X['Embarked'].isna().sum())\nprint(nan_columns)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T03:08:48.369047Z","iopub.execute_input":"2024-06-21T03:08:48.369454Z","iopub.status.idle":"2024-06-21T03:08:48.389812Z","shell.execute_reply.started":"2024-06-21T03:08:48.369422Z","shell.execute_reply":"2024-06-21T03:08:48.388760Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n(889, 11)\n177\n687\n0\n['Age', 'Cabin']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Feature Selection\n\n-- <font color = 'green'>Finding relevant features in the dataset X which will influence the Y value </font>","metadata":{}},{"cell_type":"code","source":"#the dtypes gives the data types of each variable\nprint (X.dtypes)\n# from the above we get name,sex, ticket, cabin and embarked as object data types.\n#It is clear that name does not need to be one hot encoded as it doesn't result in any\n#useful prediction. \n#To confirm embarked, cabin and sex lets perform a group by search to confirm if they \n#can be one hot encoded.\n\n# doing a distinct search in categorical features for one hot encoding.\n\ncabin_count = pd.unique(X['Cabin'])\nprint (\"Cabin, \", X[\"Cabin\"].unique())\n#print (len(cabin_count))\n#print (len(X))\nprint (\"Sex: \", X[\"Sex\"].unique())\nprint (\"Pclass: \", X[\"Pclass\"].unique()) #categorical feature one hot encode pending\nprint(\"SibSp: \", X[\"SibSp\"].unique()) #categorical feature one hot encode pending\nprint (\"Parch: \", X[\"Parch\"].unique())\n\n\nprint(X[\"Embarked\"].isna().sum())\nprint (X[\"Cabin\"].isna().sum())\nprint (X[\"Age\"].isna().sum())\n\nprint(X.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T03:06:27.894154Z","iopub.execute_input":"2024-06-21T03:06:27.894541Z","iopub.status.idle":"2024-06-21T03:06:27.908368Z","shell.execute_reply.started":"2024-06-21T03:06:27.894509Z","shell.execute_reply":"2024-06-21T03:06:27.907195Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"PassengerId      int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\nCabin,  [nan 'C85' 'C123' 'E46' 'G6' 'C103' 'D56' 'A6' 'C23 C25 C27' 'B78' 'D33'\n 'B30' 'C52' 'C83' 'F33' 'F G73' 'E31' 'A5' 'D10 D12' 'D26' 'C110'\n 'B58 B60' 'E101' 'F E69' 'D47' 'B86' 'F2' 'C2' 'E33' 'B19' 'A7' 'C49'\n 'F4' 'A32' 'B4' 'B80' 'A31' 'D36' 'D15' 'C93' 'C78' 'D35' 'C87' 'B77'\n 'E67' 'B94' 'C125' 'C99' 'C118' 'D7' 'A19' 'B49' 'D' 'C22 C26' 'C106'\n 'C65' 'E36' 'C54' 'B57 B59 B63 B66' 'C7' 'E34' 'C32' 'B18' 'C124' 'C91'\n 'E40' 'T' 'C128' 'D37' 'B35' 'E50' 'C82' 'B96 B98' 'E10' 'E44' 'A34'\n 'C104' 'C111' 'C92' 'E38' 'D21' 'E12' 'E63' 'A14' 'B37' 'C30' 'D20' 'B79'\n 'E25' 'D46' 'B73' 'C95' 'B38' 'B39' 'B22' 'C86' 'C70' 'A16' 'C101' 'C68'\n 'A10' 'E68' 'B41' 'A20' 'D19' 'D50' 'D9' 'A23' 'B50' 'A26' 'D48' 'E58'\n 'C126' 'B71' 'B51 B53 B55' 'D49' 'B5' 'B20' 'F G63' 'C62 C64' 'E24' 'C90'\n 'C45' 'E8' 'B101' 'D45' 'C46' 'D30' 'E121' 'D11' 'E77' 'F38' 'B3' 'D6'\n 'B82 B84' 'D17' 'A36' 'B102' 'B69' 'E49' 'C47' 'D28' 'E17' 'A24' 'C50'\n 'B42' 'C148']\nSex:  ['male' 'female']\nPclass:  [3 1 2]\nSibSp:  [1 0 3 4 2 5 8]\nParch:  [0 1 2 5 3 4 6]\n0\n687\n177\n(889, 11)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### One hot encoding\n\n--<font color = 'green'> based on the above code the features Cabin, Sex and Embarked are categorical features, so lets convert them to numerical data with one hot encoding. </font>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nimport numpy as np\n\ndef data_preprocessing(X):\n    #Drop \n    X = X.drop(columns=[\"PassengerId\",\"Name\", \"Ticket\", \"Cabin\",\"Parch\"])\n    #nan_columns_infun = X.columns[X.isna().any()].tolist()\n    #print (\"Before removing NAN columns: \",nan_columns_infun)\n    \n    ## dealing with the Nan values\n    age_mean = X[\"Age\"].mean()\n    X[\"Age\"] = X[\"Age\"].fillna(age_mean)\n    \n    ## Step 2: Scale the numerical features to lie between 0 and 1\n    # I am using MinMax scaler.\n    scaler = MinMaxScaler()\n    scaled_features = scaler.fit_transform(X[[\"Age\", \"Fare\"]])\n    scaled_df = pd.DataFrame(scaled_features, columns=[\"Age\", \"Fare\"])\n    X = X.drop(columns=[\"Age\", \"Fare\"])\n    X = pd.concat([X.reset_index(drop=True), scaled_df.reset_index(drop=True)], axis=1)\n    #X = pd.concat([X,scaled_df], axis=1)\n    \n    #nan_columns_infun = X.columns[X.isna().any()].tolist()\n    #print (\"NAN columns: \",nan_columns_infun)\n    \n    ##Step 1: One hot encoding categorical features\n    onehot_encode = OneHotEncoder(sparse = False, handle_unknown='ignore') #print (onehot_encode)\n    \n    #feature_array = onehot_encode.fit_transform(X[[\"Pclass\",\"Sex\",\"SibSp\",\"Parch\",\"Embarked\"]]).toarray()\n    feature_array = onehot_encode.fit_transform(X[[\"Pclass\",\"Sex\",\"SibSp\",\"Embarked\"]])\n    #feature_labels = onehot_encode.categories_ \n    feature_labels = onehot_encode.get_feature_names_out([\"Pclass\",\"Sex\",\"SibSp\",\"Embarked\"])\n    print(feature_labels)\n    \n    #dropping the encoded features\n    X = X.drop(columns = [\"Pclass\",\"Sex\",\"SibSp\",\"Embarked\"])\n    \n    #flattened_label = []\n    #for sublist in feature_labels:\n    #    for items in sublist:\n    #        flattened_label.append(items)\n    \n    #print (flattened_label)\n    features = pd.DataFrame(feature_array, columns = feature_labels)\n    X = pd.concat([X,features], axis = 1)\n    nan_columns_infun = X.columns[X.isna().any()].tolist()\n    print (nan_columns_infun)\n    print (X.shape)\n    return X\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T03:14:04.293008Z","iopub.execute_input":"2024-06-21T03:14:04.293411Z","iopub.status.idle":"2024-06-21T03:14:04.305908Z","shell.execute_reply.started":"2024-06-21T03:14:04.293379Z","shell.execute_reply":"2024-06-21T03:14:04.304723Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"X_new = data_preprocessing(X)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T03:14:07.397950Z","iopub.execute_input":"2024-06-21T03:14:07.398448Z","iopub.status.idle":"2024-06-21T03:14:07.421983Z","shell.execute_reply.started":"2024-06-21T03:14:07.398406Z","shell.execute_reply":"2024-06-21T03:14:07.420648Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"['Pclass_1' 'Pclass_2' 'Pclass_3' 'Sex_female' 'Sex_male' 'SibSp_0'\n 'SibSp_1' 'SibSp_2' 'SibSp_3' 'SibSp_4' 'SibSp_5' 'SibSp_8' 'Embarked_C'\n 'Embarked_Q' 'Embarked_S']\n[]\n(889, 17)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"#print(type(X_new))\n#print (type(Y))\nprint (X_new)\nprint (X_new.columns)\n\nnan_columns = X.columns[X.isna().any()].tolist()\nprint (nan_columns)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T03:14:15.088572Z","iopub.execute_input":"2024-06-21T03:14:15.088977Z","iopub.status.idle":"2024-06-21T03:14:15.109200Z","shell.execute_reply.started":"2024-06-21T03:14:15.088946Z","shell.execute_reply":"2024-06-21T03:14:15.108086Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"          Age      Fare  Pclass_1  Pclass_2  Pclass_3  Sex_female  Sex_male  \\\n0    0.271174  0.014151       0.0       0.0       1.0         0.0       1.0   \n1    0.472229  0.139136       1.0       0.0       0.0         1.0       0.0   \n2    0.321438  0.015469       0.0       0.0       1.0         1.0       0.0   \n3    0.434531  0.103644       1.0       0.0       0.0         1.0       0.0   \n4    0.434531  0.015713       0.0       0.0       1.0         0.0       1.0   \n..        ...       ...       ...       ...       ...         ...       ...   \n884  0.334004  0.025374       0.0       1.0       0.0         0.0       1.0   \n885  0.233476  0.058556       1.0       0.0       0.0         1.0       0.0   \n886  0.367204  0.045771       0.0       0.0       1.0         1.0       0.0   \n887  0.321438  0.058556       1.0       0.0       0.0         0.0       1.0   \n888  0.396833  0.015127       0.0       0.0       1.0         0.0       1.0   \n\n     SibSp_0  SibSp_1  SibSp_2  SibSp_3  SibSp_4  SibSp_5  SibSp_8  \\\n0        0.0      1.0      0.0      0.0      0.0      0.0      0.0   \n1        0.0      1.0      0.0      0.0      0.0      0.0      0.0   \n2        1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n3        0.0      1.0      0.0      0.0      0.0      0.0      0.0   \n4        1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n..       ...      ...      ...      ...      ...      ...      ...   \n884      1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n885      1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n886      0.0      1.0      0.0      0.0      0.0      0.0      0.0   \n887      1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n888      1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n\n     Embarked_C  Embarked_Q  Embarked_S  \n0           0.0         0.0         1.0  \n1           1.0         0.0         0.0  \n2           0.0         0.0         1.0  \n3           0.0         0.0         1.0  \n4           0.0         0.0         1.0  \n..          ...         ...         ...  \n884         0.0         0.0         1.0  \n885         0.0         0.0         1.0  \n886         0.0         0.0         1.0  \n887         1.0         0.0         0.0  \n888         0.0         1.0         0.0  \n\n[889 rows x 17 columns]\nIndex(['Age', 'Fare', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female',\n       'Sex_male', 'SibSp_0', 'SibSp_1', 'SibSp_2', 'SibSp_3', 'SibSp_4',\n       'SibSp_5', 'SibSp_8', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n['Age', 'Cabin']\n","output_type":"stream"}]},{"cell_type":"code","source":"#print ('Training set :', X_new)\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data_preprocessed = data_preprocessing(test_data)\nfare_mean = test_data_preprocessed['Fare'].mean()\ntest_data_preprocessed['Fare'] = test_data_preprocessed['Fare'].fillna(fare_mean)\nprint ('Test data :', test_data_preprocessed.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T03:14:21.531026Z","iopub.execute_input":"2024-06-21T03:14:21.531415Z","iopub.status.idle":"2024-06-21T03:14:21.557758Z","shell.execute_reply.started":"2024-06-21T03:14:21.531384Z","shell.execute_reply":"2024-06-21T03:14:21.556651Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"['Pclass_1' 'Pclass_2' 'Pclass_3' 'Sex_female' 'Sex_male' 'SibSp_0'\n 'SibSp_1' 'SibSp_2' 'SibSp_3' 'SibSp_4' 'SibSp_5' 'SibSp_8' 'Embarked_C'\n 'Embarked_Q' 'Embarked_S']\n['Fare']\n(418, 17)\nTest data : (418, 17)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid_randomForest = {\n    'n_estimators' : [200, 300, 400, 500],\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 9, 11]\n}\nmodel = RandomForestClassifier()\n\n#initialize gridSearchCV\ngrid_search = GridSearchCV(estimator = model, param_grid = param_grid_randomForest, cv = 5)\nX_new.columns = X_new.columns.astype(str)\n#Fit the model to train data\ngrid_search.fit (X_new, Y)\n#Get the best model \nbest_model = grid_search.best_estimator_\ntest_data_preprocessed.columns = test_data_preprocessed.columns.astype(str)\ny_pred = best_model.predict(test_data_preprocessed)\n\n#calculate the accuracy\n#accuracy = accuracy_score (Y_test, y_pred)\n#print(accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T02:57:35.707627Z","iopub.execute_input":"2024-06-21T02:57:35.708193Z","iopub.status.idle":"2024-06-21T02:59:55.062421Z","shell.execute_reply.started":"2024-06-21T02:57:35.708156Z","shell.execute_reply":"2024-06-21T02:59:55.061387Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"#lets try with XGBoost\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n\nX_new.columns = X_new.columns.astype(str)\nparam_grid_xgboost = {\n    'n_estimators' : [100, 200, 300, 400],\n    'max_depth' : [3, 5, 7, 9]\n}\n#initialize gridSearchCV\nmodel = XGBClassifier()\n\ngrid_search = GridSearchCV(estimator = model, param_grid = param_grid_xgboost, cv = 5)\n\ngrid_search.fit(X_new, Y)\ntest_data_preprocessed.columns = test_data_preprocessed.columns.astype(str)\n#Get the best model \nbest_model_xgb = grid_search.best_estimator_\ny_pred = best_model_xgb.predict(test_data_preprocessed)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T03:29:57.019917Z","iopub.execute_input":"2024-06-21T03:29:57.020333Z","iopub.status.idle":"2024-06-21T03:30:11.297392Z","shell.execute_reply.started":"2024-06-21T03:29:57.020303Z","shell.execute_reply":"2024-06-21T03:30:11.296349Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"print (y_pred)\ntest_data_columns = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nsubmission = pd.DataFrame({'PassengerId':test_data_columns['PassengerId'],'Survived':y_pred})\n#Visualize the first 5 rows\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T03:31:01.597292Z","iopub.execute_input":"2024-06-21T03:31:01.597658Z","iopub.status.idle":"2024-06-21T03:31:01.615200Z","shell.execute_reply.started":"2024-06-21T03:31:01.597628Z","shell.execute_reply":"2024-06-21T03:31:01.613879Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"[0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1\n 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0\n 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0\n 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0\n 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n 0 0 1 0 1 1 0 1 0 0 0]\n","output_type":"stream"},{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Survived\n0          892         0\n1          893         0\n2          894         0\n3          895         0\n4          896         0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Convert DataFrame to a csv file that can be uploaded\n#This is saved in the same directory as your notebook\nfilename = 'submission.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T03:31:07.262617Z","iopub.execute_input":"2024-06-21T03:31:07.263038Z","iopub.status.idle":"2024-06-21T03:31:07.271235Z","shell.execute_reply.started":"2024-06-21T03:31:07.263009Z","shell.execute_reply":"2024-06-21T03:31:07.270115Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stdout","text":"Saved file: submission.csv\n","output_type":"stream"}]}]}